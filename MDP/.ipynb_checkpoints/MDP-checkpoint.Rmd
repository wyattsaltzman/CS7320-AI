---
title: "MDP - Markov Decision Processes"
author: "Michael Hahsler"
output: 
  html_document: 
    df_printed: paged
    toc: yes
---

This code is provided under [Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License.](https://creativecommons.org/licenses/by-sa/4.0/)

![CC BY-SA 4.0](https://licensebuttons.net/l/by-sa/3.0/88x31.png)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
```

# Introduction

AIMA chapter 17 is about _sequential decision problems_ where the agent's utility depends on a sequence of decisions. We will implement the 
key concepts using R. 
**Note** that the code in this notebook defines explicit functions 
matching the textbook definitions and is for demonstration purposes only. Efficient implementations for larger problems use fast vector multiplications
instead.

The used example is described in AIMA Figure 17.1 as: 

![AIMA Figure 17.1: (a) A simple, stochastic $4 \times 3$  environment that presents the
agent with a sequential decision problem. (b) Illustration of the
transition model of the environment: the "intended" outcome occurs with
probability 0.8, but with probability 0.2 the agent moves at right
angles to the intended direction. A collision with a wall results in no
movement. Transitions into the two terminal states have reward +1 and
-1, respectively, and all other transitions have a reward of -0.04.](AIMA_Figure_17_1.png)





# Defining a Markov Decision Process

MDPs are sequential decision problems with

-   a fully observable, stochastic environment,
-   a Markovian transition model, and
-   additive rewards.

MDPs are defined by:

-   A set of states $S$ with an initial state $s_0$.
-   A set of available $\mathrm{actions}(s)$ in each state.
-   A transition model $P(s'|s,a)$ to define how we move between states depending on actions.
-   A reward function $R(s, a, s')$ defined on state transitions and the actions taken.

A **policy** $\pi = \{\pi(s_0), \pi(s_1), \dots\}$ defines for each state which action to take. If we assume that under policy $\pi$, the agent
will go through the state sequence
$s_0, s_1, ..., s_\infty$, then the expected utility of being in state
$s_0$ can be calculated as a sum. To incorporate that earlier rewards
are more important, a discount factor $\gamma$ is used.

$U^\pi(s_0) = E\left[\sum_{t=0}^\infty \gamma^t R(s_t, \pi(s_t), s_{t+1})\right]$

The goal of solving a MDP is to find an optimal policy that maximizes the expected future utility.

$\pi^*_{s_0} = \mathrm{argmax}_\pi U^\pi(s_0)$

Note that this is the policy if the agent starts in state $s_0$. Interestingly, under some mild conditions, the policy does not depend on the start state and can be written as simply $\pi^*$.

# Implement MDPs




## States

We define the atomic state space $S$ by labeling the states $1, 2, ...$.
We convert coordinates `(rows, columns)` to the state label.

```{r}
# I use capitalized variables as global constants
COLS <- 4
ROWS <- 3

S = seq_len(ROWS * COLS)

LAYOUT <- matrix(S, nrow = ROWS , ncol = COLS)
LAYOUT
```

Note that the rows are displayed upside-down compared to the text book,
so we use a function to display them in reverse order.

```{r}
show_layout <- function(x) {
  x <- matrix(x, ncol = COLS, nrow = ROWS, 
    dimnames = list(row = seq_len(ROWS), col = seq_len(COLS)))
  x[rev(seq_len(ROWS)), ]
  }

show_layout(LAYOUT)
```

Convert between coordinates and state labels.

```{r}
rc_to_s <- function(rc)
  LAYOUT[rbind(rc)]

s_to_rc <-
  function(s)
    drop(which(LAYOUT == s, arr.ind = TRUE, useNames = FALSE))
  
    
rc_to_s(c(3, 4))
s_to_rc(12)
```

## Actions

The complete set of actions is
$A = \{\mathrm{'Up', 'Right', 'Down', 'Left', 'None'}\}$. Not all
actions are available in every state. Also, action `None` is added as
the only possible action in an absorbing state.

```{r}
A = c('Up', 'Right', 'Down', 'Left', 'None')

actions <- function(s) { 
  
  # absorbing states
  if(s == 11 || s == 12) return('None')
  
  # illegal state
  if(s == 5) return('None')
  
  # regular states
  rc <- s_to_rc(s)
  c(
    if(rc[1] < ROWS) 'Up', 
    if(rc[2] < COLS) 'Right',
    if(rc[1] > 1) 'Down',
    if(rc[2] > 1) 'Left'
  )
}
     
lapply(S, actions)
```

## Transition model

$P(s' | s, a)$ is the probability of going from state $s$ to $s'$ by
when taking action $a$. We will create a matrix $P_a(s' | s)$ for each
action.

```{r}
calc_transition <- function(s, action) {
  action <- match.arg(action, choices = A)
  
  if(length(s) > 1) return(t(sapply(s, calc_transition, action = action)))
  
  # deal with absorbing and illegal state
  if(s == 11 || s == 12 || s == 5 || action == 'None') {
    P <- rep(0, length(S))
    P[s] <- 1
    return(P)    
  }
  
  action_to_delta <- list(
    'Up' = c(+1, 0),
    'Down' = c(-1, 0),
    'Right' = c(0, +1),
    'Left' = c(0, -1)
    )
  delta <- action_to_delta[[action]]
  dr <- delta[1]
  dc <- delta[2]
   
  rc <- s_to_rc(s)
  r <- rc[1]
  c <- rc[2]
  
  if(dr != 0 && dc != 0) 
    stop("You can only go up/down or right/left!")
  
  P <- matrix(0, nrow = ROWS, ncol = COLS)
  
  # UP/DOWN
  if(dr != 0) {
    new_r <- r + dr
    if(new_r > ROWS || new_r < 1) new_r <- r
    ## can't got to (2, 2)
    if(new_r == 2 && c  == 2) new_r <- r
    P[new_r, c] <- .8
    
    if(c < COLS & !(r == 2 & (c + 1) == 2)) 
      P[r, c + 1] <- .1 else P[r, c] <- P[r, c] + .1 
    if(c > 1 & !(r == 2 & (c - 1) == 2)) 
      P[r, c - 1] <- .1 else P[r, c] <- P[r, c] + .1 
  }
  
  # RIGHT/LEFT
  if(dc != 0) {
    new_c <- c + dc
    if(new_c > COLS || new_c < 1) new_c <- c
    ## can't got to (2, 2)
    if(r == 2 && new_c  == 2) new_c <- c
    P[r, new_c] <- .8
    
    if(r < ROWS & !((r + 1) == 2 & c  == 2)) 
      P[r + 1, c] <- .1 else P[r, c] <- P[r, c] + .1 
    if(r > 1 & !((r - 1) == 2 & c == 2)) 
      P[r - 1, c] <- .1 else P[r, c] <- P[r, c] + .1 
  }
  
  as.vector(P)
}

calc_transition(1, 'Up')
show_layout(calc_transition(1, 'Up'))

### can't go to (2, 2)
show_layout(calc_transition(2, 'Right'))
```

Calculate transitions for each state to each other state. Each row
represents a state $s$ and each column a state $s'$ so we get a complete
definition for $P_a(s' | s)$. Note that the matrix is stochastic (all
rows add up to 1).

```{r}
calc_transition(S, action = 'Right')
```

Create a matrix for each action.

```{r}
P_matrices <- lapply(A, FUN = function(a) calc_transition(S, a))
names(P_matrices) <- A
str(P_matrices)
```

Create a function interface for $P(s' | s, a)$.

```{r}
P <- function(sp, s, a) P_matrices[[a]][s, sp]

P(2, 1, 'Up')
P(5, 4, 'Up')
```

## Rewards

$R(s, a, s')$ define the reward for the transition from $s$ to $s'$ with
action $a$.

For the textbook example we have:

-   Any move costs utility (a reward of -0.04).
-   Going to state 12 has a reward of +1
-   Going to state 11 has a reward of -1.

Note that once you are in an absorbing state (11 or 12), then the
problem is over and there is no more reward!

**Note:** The textbook uses the reward of -0.04 to make the agent get to
the goal as fast as possible when no discounting is used. The
discounting below already does this, so we could use 0 instead.

```{r}
R <- function(s, a, s_prime) {
  ## no more reward when we in 11 or 12.
  if(a == 'None' || s == 11 || s == 12) return(0)
  
  ## transition to the absorbing states.
  if(s_prime == 12) return(+1)
  if(s_prime == 11) return(-1)
  
  ## cost for each move
  return(-0.04)
}

R(1, 'Up', 2)
R(9, 'Right', 12)
R(12, 'None', 12)
```

## Policy
The solution to an MDP is a policy $\pi$ which defines which action
to take in each state. We represent it as a vector.
I make up a policy that always goes up and then to the right once the
agent hits the top.

```{r}
pi_manual <- rep('Up', times = length(S))
pi_manual[c(3, 6, 9)] <- 'Right'
show_layout(pi_manual)
```

We can also create a random policy by randomly choosing from the
available actions for each state.

```{r}
set.seed(1234)
create_random_policy <-
  function()
    sapply(
      S,
      FUN = function(s)
        sample(actions(s), 1)
    )

pi_random <- create_random_policy()
show_layout(pi_random)
```

## Expected Utility

The expected utility can be calculated by

$U^\pi(s_0) = E\left[\sum_{t=0}^\infty \gamma^t R(s_t, \pi(s_t), s_{t+1})\right]$

We need to define the discount factor.

```{r}
GAMMA <- 0.9
```

We can evaluate the utility of a policy using simulation. We need to be able
to sample state transitions when using action $a$ in state $s$.

```{r}
sample_transition <- function(s, a)
  sample(S, size = 1, prob = P_matrices[[a]][s,])

sample_transition(1, 'Up')

table(replicate(n = 100, sample_transition(1, 'Up')))
```

```{r}
simulate_utility <- function(pi, s0 = 1) {
  s <- s0
  U <- 0
  t <- 0
  
  while (TRUE) {
    ## get action from policy
    a <- pi[s]
    
    ## sample a transition given the action from the policy
    s_prime <- sample_transition(s, a)
    
    ##
    U <- U + GAMMA ^ t * R(s, a, s_prime)
    
    s <- s_prime
    t <- t + 1
    
    ## reached an absorbing state?
    if (s == 11 || s == 12 || s == 5)
      break
  }
  
  U
}

simulate_utilities <- function(pi, s0 = 1, N = 1000)
  replicate(N, simulate_utility(pi, s0))
```

The expected utility for starting from state $s_0 = 1$ is.

```{r}
utility_manual <- simulate_utilities(pi_manual)

# expected utility
mean(utility_manual)
hist(utility_manual, xlim = c(-1, 1))
```

Compare with the random policy.
```{r}
utility_random <- simulate_utilities(pi_random)

# expected utility
mean(utility_random)
hist(utility_random, xlim = c(-1, 1))
```

The manually created policy is obviously better.

We can use simulation to estimate the expected utility for starting from 
each state following the policy.

```{r}
U_manual <-
  sapply(
    S,
    FUN = function(s)
      mean(simulate_utilities(pi_manual, s0 = s))
  )
show_layout(U_manual)
```

```{r}
U_random <-
  sapply(
    S,
    FUN = function(s)
      mean(simulate_utilities(pi_random, s0 = s))
  )
show_layout(U_random)
```

## Q-Function

Let's define the expected utility of a state given that the agent always
chooses the optimal action (which it hopefully will if it is rational).

$U(s) = \max_{a \in A(s)}\sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma U(s')]$

This equation is called the _Bellman equation_ resulting in an equation system
with one equation per state $s$. This system of equations is hard to solve
for all $U(s)$ values because of the nonlinear $\max()$ operator.

Lets define a function for the expected utility of any possible
action $a$ (not just the optimal one) in a given state $s$. This is
called the (Q-function (or action-utility function):

$Q(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma U(s')]$

Note that $U(s) = \max_a Q(s, a)$ holds and we get:

$Q(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \max_{a'} Q(s', a')]$

This function is convenient for solving MDPs and can easily be implemented.

```{r}
Q_value <- function(s, a, U) {
  if(!(a %in% actions(s))) return(NA)
  
  sum(sapply(
    S,
    FUN = function(sp)
      P(sp, s, a) * (R(s, a, sp) + GAMMA * U[sp])
  ))
}
```

The issue is that the definitions are recursive and the unknown $U(s)$
representing the expected utility of a state given optimal decisions
is needed to calculate $U(s)$. Value iteration uses a simple iterative 
algorithm to solve this problem.

# Solution Methods

## Value Iteration

The goal is to find the unique utility function $U(s)$ (a vector of 
utilities, one for each state) for the MDP and
then derive the implied optimal policy $\pi^*$.

**Algorithm:** Start with a $U(s)$ vector of 0 for all states and then
update (Bellman update) the vector iteratively until it converges. This
procedure is guaranteed to converge to the unique optimal solution.

**Stopping criterion:**
$||U^\pi - U_||_\infty$ is called the _policy loss_ (i.e., the most the agent can loose by using policy $\pi$ instead of 
the optimal policy $\pi^*$ implied in $U$).
The max-norm $||x||_\infty$ is defined as the largest component of a 
vector $x$. 

It can be shown that if
$||U_{i+1} - U_i||_\infty < \epsilon(1-\gamma)/\gamma$ then
$||U_{i+1} - U||_\infty < \epsilon$. This can be used as a stopping 
criterion with guarantee of a policy loss of less than $\epsilon$.

```{r}
value_iteration <- function(eps, verbose = FALSE) {
  U_prime <- rep(0, times = length(S))
  i <- 1L
   
  while (TRUE) {
    if(verbose) cat("Iteration:", i)
    #cat("U:", U_prime, "\n")
    
    U <- U_prime
    delta <- 0
    
    for (s in S) {
      U_prime[s] <- max(sapply(
        actions(s),
        FUN = function(a)
          Q_value(s, a, U)
      ))
      delta <- max(delta, abs(U_prime[s] - U[s]))
    }
    
    if(verbose) cat(" -> delta:", delta, "\n")
    
    if (delta <= eps * (1 - GAMMA) / GAMMA)
      break
    
  i <- i + 1L  
  }
  
  cat("Iterations needed:", i, "\n")
  
  U
}
```

```{r}
U <- value_iteration(eps = 0.001)
show_layout(U)
```

_Note:_ You can set `GAMMA <- 0` to replicate the results from the textbook (Figure 17.3).

For the optimal policy, we choose in each state the action that
maximizes the expected utility. This is called the maximum expected utility (MEU) policy. The action that maximizes the utility can be found using the 
Q-function.

$\pi^*(s) = \mathrm{argmax}_a Q(s, a)$

For state 1, `'Up'` is the best move

```{r}
sapply(A, FUN = function(a) Q_value(s = 1, a, U = U))
```

Calculate the Q-function for all $S \times A$ and pick the best for each
state.

```{r}
Q_value_vec <- Vectorize(Q_value, vectorize.args = c("s", "a"))

QVs <- outer(S, A, FUN = function(s, a) Q_value_vec(s, a, U = U))
colnames(QVs) <- A

pi_star <- A[apply(QVs, MARGIN = 1, which.max)]
```

Here is the optimal policy:

```{r}
show_layout(pi_star)
```

Estimate the expected utility using simulation.

```{r}
utility_opt <- simulate_utilities(pi_star)

# expected utility
mean(utility_opt)
hist(utility_opt, xlim = c(-1, 1))
```

Compare three policies.

```{r}
c(
  random = mean(utility_random), 
  manual = mean(utility_manual), 
  opt = mean(utility_opt))
```

Since we know that utility_opt is very close to $U$, we can estimate the 
_policy loss_ (i.e., the most the agent can loose by using 
$\pi$ instead of $\pi*$) of the other policies given by:

$||U^\pi - U||_\infty$

Here is the policy loss for the manual policy. The maximum norm is the 
component with the largest difference. First, we calculate the absolute difference for each state.

```{r}
show_layout(abs(U_manual - U)) 
```

The maximum is:

```{r}
max(abs(U_manual - U))
which.max(abs(U_manual - U))
```

The policy loss is driven by the bad action taken in state 10 which is at coordinate (1, 4).

## Policy Iteration

Policy iteration tries to directly find the optimal policy. It alternates
between two steps:

1. **Policy evaluation:** given a current policy $\pi_i$, calculate $U^{\pi_i}$.
2. **Policy improvement:** calculate a new MEU policy $\pi_{i+1}$.

For policy evaluation, we need to solve:

$U_i(s) = \sum_{s'} P(s'|s, \pi_i(s))[R(s, \pi_i(s), s') + \gamma U_i(s')]$

This is slightly simpler than the general Bellman equation, since 
the action in each state is fixed by the policy and there is no non-linear $\max()$ operator. For small state spaces this can be solved fast using a LP in $O(n^3)$.

For large state spaces, we can do approximate policy evaluation
by performing a few iterations of a simplified Bellman update:

$U_{i+1}(s) \leftarrow \sum_{s'} P(s'|s, \pi_i(s))[R(s, \pi_i(s), s') + \gamma U_i(s')]$

```{r}
approx_policy_evaluation <- function(pi, U = NULL, N = 10) {
  # start with all 0s if no previous U is given
  if (is.null(U))
    U <- rep(0, times = length(S))
  
  for (i in seq_len(N)) {
    for (s in S) {
      U[s] = sum(sapply(
        S,
        FUN = function(s_prime) {
          P(s_prime, s, pi[s]) * (R(s, pi[s], s_prime) + GAMMA * U[s_prime])
        }
      ))
    }
  }
  U
}
```

```{r}
approx_policy_evaluation(pi_random)
approx_policy_evaluation(pi_manual)
```

We will implement modified policy iteration. Modified means that we use
the approximate plicy evaluation.

```{r}
policy_iteration <- function() {
  U <- rep(0, times = length(S))
  pi <- create_random_policy()
  
  while (TRUE) {
    U <- approx_policy_evaluation(pi, U)
    unchanged <- TRUE
    for (s in S) {
      actns <- actions(s)
      a <- actns[which.max(sapply(actns, FUN = function(a) Q_value(s, a, U)))]
      if (Q_value(s, a, U) > Q_value(s, pi[s], U)) {
        pi[s] <- a
        unchanged <- FALSE
      }
    }
    
    if(unchanged) break
  }
  pi
}
```

```{r}
pi_opt_policy_it <- policy_iteration()
show_layout(pi_opt_policy_it)
```





## Linear Programming

Rewriting the Bellman equations as an LP formulation
requires replacing the non-linear $\max()$ operations using 
additional constraints. The LP can be solved in polynomial time. In practice
this is too slow for larger problems. Dynamic programming is typically
more efficient, but it is also restricted to small problems. 

## Approximate Offline Methods

Reinforcement learning is discussed in AIMA Chapter 22.
